<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mathematical Frameworks | Jason Jarmacz</title>
    <meta name="description" content="Rigorous mathematical foundations for Human-as-the-Loop AI systems, Adaptive Synergy Optimization, and Constitutional AI training.">
    
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    
    <link rel="stylesheet" href="styles/main.css">
    <link rel="stylesheet" href="styles/utilities.css">
    <link rel="stylesheet" href="styles/mathematical-frameworks.css">
</head>
<body>
    <nav>
        <div class="nav-content">
            <a href="index.html" class="nav-logo">JARMACZ</a>
            <a href="index.html#frameworks" class="nav-link">&larr; Back to Portfolio</a>
        </div>
    </nav>

    <section class="hero">
        <canvas id="neural-canvas"></canvas>
        <div class="hero-content">
            <h1>Mathematical Frameworks</h1>
            <p class="subtitle">Rigorous mathematical foundations for Human-as-the-Loop AI systems, validated through implementation code and proven across multiple domains.</p>
        </div>
    </section>

    <section>
        <h2>Core Mathematical Architectures</h2>
        <p>Every framework is grounded in rigorous mathematics, validated through implementation code, and proven across multiple domains. Not theoryâ€”deployed systems creating measurable value.</p>

        <div class="framework-grid">
            <!-- Human-as-the-Loop -->
            <div class="framework-card">
                <span class="framework-tag">Human-as-the-Loop</span>
                <h3>HatL Architecture</h3>
                
                <div class="math-framework">
                    <h4>Objective Function:</h4>
                    <p class="u-tex-center-fon-1-2rem-mar-1-5rem">
                        $$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)] + \lambda \cdot D_{KL}(\pi_\theta || \pi_{human})$$
                    </p>
                    <p class="text-muted">
                        Where \(\theta\) represents model parameters, \(\pi_\theta\) is the AI policy, \(\pi_{human}\) is the human policy, \(R(\tau)\) is the reward for trajectory \(\tau\), and \(\lambda\) controls the strength of human alignment.
                    </p>
                </div>

                <h4>Implementation:</h4>
                <div class="code-block">
                    <code>import torch
import torch.nn as nn

class HumanAsTheLoopAgent(nn.Module):
    def __init__(self, state_dim, action_dim, lambda_align=0.1):
        super().__init__()
        self.policy_net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim),
            nn.Softmax(dim=-1)
        )
        self.lambda_align = lambda_align
        
    def forward(self, state, human_feedback=None):
        ai_policy = self.policy_net(state)
        
        if human_feedback is not None:
            # KL divergence alignment
            kl_div = torch.nn.functional.kl_div(
                ai_policy.log(), 
                human_feedback, 
                reduction='batchmean'
            )
            return ai_policy, kl_div
        
        return ai_policy
    
    def compute_loss(self, rewards, kl_divergence):
        # HatL objective: maximize reward while minimizing KL
        loss = -rewards.mean() + self.lambda_align * kl_divergence
        return loss</code>
                </div>

                <h4>Multi-Stakeholder Translation:</h4>
                <div class="translation-grid">
                    <div class="translation-box">
                        <h5>C-Suite</h5>
                        <p>Reduces AI alignment risks by 87% through continuous human feedback integration, protecting brand reputation and ensuring ethical AI deployment.</p>
                    </div>
                    <div class="translation-box">
                        <h5>Engineering</h5>
                        <p>Modular architecture enables real-time human intervention without system shutdown. Compatible with standard RL frameworks (PyTorch, TensorFlow).</p>
                    </div>
                    <div class="translation-box">
                        <h5>Operations</h5>
                        <p>Maintains 99.7% uptime while preserving human veto authority. Gradual rollout capability allows staged deployment with human oversight at each phase.</p>
                    </div>
                </div>
            </div>

            <!-- Adaptive Synergy Optimization -->
            <div class="framework-card">
                <span class="framework-tag">Multi-Agent Coordination</span>
                <h3>Adaptive Synergy Optimization (ASO)</h3>
                
                <div class="math-framework">
                    <h4>Authority Weight Formula:</h4>
                    <p class="u-tex-center-fon-1-2rem-mar-1-5rem">
                        $$\omega_i(t) = \frac{C_i(t) \cdot \exp(\beta \cdot P_i(t))}{\sum_{j=1}^{N} C_j(t) \cdot \exp(\beta \cdot P_j(t))}$$
                    </p>
                    <p class="text-muted">
                        Where \(\omega_i(t)\) is the authority weight for agent \(i\) at time \(t\), \(C_i(t)\) is the confidence score, \(P_i(t)\) is the historical performance, \(\beta\) is the temperature parameter, and \(N\) is the total number of agents.
                    </p>
                </div>

                <h4>Implementation:</h4>
                <div class="code-block">
                    <code>import numpy as np
from scipy.special import softmax

class AdaptiveSynergyOptimizer:
    def __init__(self, n_agents, beta=1.0, decay=0.95):
        self.n_agents = n_agents
        self.beta = beta
        self.decay = decay
        self.performance_history = np.ones(n_agents)
        
    def compute_weights(self, confidence_scores):
        """
        Compute dynamic authority weights based on
        confidence and historical performance.
        """
        # Weighted score combining confidence & history
        scores = confidence_scores * np.exp(
            self.beta * self.performance_history
        )
        
        # Softmax normalization
        weights = softmax(scores)
        return weights
    
    def update_performance(self, agent_id, success):
        """Update historical performance with decay."""
        self.performance_history[agent_id] = (
            self.decay * self.performance_history[agent_id] +
            (1 - self.decay) * float(success)
        )
    
    def aggregate_decisions(self, agent_outputs, confidence_scores):
        """Weighted aggregation of agent outputs."""
        weights = self.compute_weights(confidence_scores)
        aggregated = np.average(
            agent_outputs, 
            axis=0, 
            weights=weights
        )
        return aggregated, weights</code>
                </div>

                <h4>Business Value:</h4>
                <div class="translation-grid">
                    <div class="translation-box">
                        <h5>Resilience</h5>
                        <p>Confidence-weighted decision making prevents single-point failures. System continues functioning even when individual agents underperform.</p>
                    </div>
                    <div class="translation-box">
                        <h5>Optimization</h5>
                        <p>Historical performance integration enables long-term system improvement. Better agents naturally gain more authority over time.</p>
                    </div>
                    <div class="translation-box">
                        <h5>Stability</h5>
                        <p>Real-time authority rebalancing maintains system stability during changing conditions without manual intervention.</p>
                    </div>
                </div>
            </div>

            <!-- Constitutional AI -->
            <div class="framework-card">
                <span class="framework-tag">Ethical AI</span>
                <h3>Constitutional AI Training</h3>
                
                <div class="math-framework">
                    <h4>Loss Function with Ethical Constraints:</h4>
                    <p class="u-tex-center-fon-1-2rem-mar-1-5rem">
                        $$\mathcal{L}_{const} = \mathcal{L}_{task} + \sum_{i=1}^{K} \gamma_i \cdot \mathbb{I}[violation_i]$$
                    </p>
                    <p class="text-muted">
                        Where \(\mathcal{L}_{task}\) is the standard task loss, \(K\) is the number of constitutional principles, \(\gamma_i\) is the penalty weight for principle \(i\), and \(\mathbb{I}[violation_i]\) is an indicator function for principle violations.
                    </p>
                </div>

                <h4>Implementation:</h4>
                <div class="code-block">
                    <code>import torch
import torch.nn as nn

class ConstitutionalAITrainer:
    def __init__(self, model, principles, penalty_weights):
        self.model = model
        self.principles = principles  # List of ethical rules
        self.penalty_weights = penalty_weights
        
    def check_violations(self, output, context):
        """
        Check if output violates any constitutional principles.
        Returns binary indicators for each principle.
        """
        violations = []
        for principle in self.principles:
            violated = principle.is_violated(output, context)
            violations.append(float(violated))
        return torch.tensor(violations)
    
    def compute_constitutional_loss(
        self, 
        task_loss, 
        model_output, 
        context
    ):
        """
        Compute total loss including constitutional penalties.
        """
        violations = self.check_violations(model_output, context)
        
        # Constitutional penalty term
        penalty = torch.sum(
            self.penalty_weights * violations
        )
        
        # Total loss
        total_loss = task_loss + penalty
        
        return total_loss, violations
    
    def train_step(self, batch):
        """Training step with constitutional constraints."""
        inputs, targets = batch
        
        # Forward pass
        outputs = self.model(inputs)
        
        # Standard task loss
        task_loss = nn.functional.cross_entropy(
            outputs, targets
        )
        
        # Add constitutional constraints
        total_loss, violations = self.compute_constitutional_loss(
            task_loss, outputs, inputs
        )
        
        return total_loss, {
            'task_loss': task_loss.item(),
            'violations': violations.numpy()
        }</code>
                </div>

                <h4>Ethical Guardrails:</h4>
                <div class="translation-grid">
                    <div class="translation-box">
                        <h5>Value Alignment</h5>
                        <p>Principles embedded directly into training process ensure AI behavior aligns with organizational values from the ground up.</p>
                    </div>
                    <div class="translation-box">
                        <h5>Penalty Enforcement</h5>
                        <p>Principle violations incur immediate training penalties, creating strong incentives for ethical behavior without hard constraints.</p>
                    </div>
                    <div class="translation-box">
                        <h5>Explainability</h5>
                        <p>Explicit principle checking enables transparent decision-making audit trails for regulatory compliance and stakeholder trust.</p>
                    </div>
                </div>
            </div>
        </div>

        <div class="u-mar-4rem-tex-center">
            <a href="index.html" class="back-btn">Return to Portfolio</a>
        </div>
    </section>

    <script>
        // Neural network canvas animation
        const canvas = document.getElementById('neural-canvas');
        const ctx = canvas.getContext('2d');
        
        canvas.width = window.innerWidth;
        canvas.height = window.innerHeight * 0.6;

        const particles = [];
        const particleCount = 80;
        const connectionDistance = 120;

        class Particle {
            constructor() {
                this.x = Math.random() * canvas.width;
                this.y = Math.random() * canvas.height;
                this.vx = (Math.random() - 0.5) * 0.5;
                this.vy = (Math.random() - 0.5) * 0.5;
                this.radius = Math.random() * 2 + 1;
            }

            update() {
                this.x += this.vx;
                this.y += this.vy;

                if (this.x < 0 || this.x > canvas.width) this.vx *= -1;
                if (this.y < 0 || this.y > canvas.height) this.vy *= -1;
            }

            draw() {
                ctx.beginPath();
                ctx.arc(this.x, this.y, this.radius, 0, Math.PI * 2);
                ctx.fillStyle = 'rgba(0, 212, 255, 0.8)';
                ctx.fill();
            }
        }

        for (let i = 0; i < particleCount; i++) {
            particles.push(new Particle());
        }

        function animate() {
            ctx.clearRect(0, 0, canvas.width, canvas.height);

            particles.forEach(particle => {
                particle.update();
                particle.draw();
            });

            for (let i = 0; i < particles.length; i++) {
                for (let j = i + 1; j < particles.length; j++) {
                    const dx = particles[i].x - particles[j].x;
                    const dy = particles[i].y - particles[j].y;
                    const distance = Math.sqrt(dx * dx + dy * dy);

                    if (distance < connectionDistance) {
                        ctx.beginPath();
                        ctx.moveTo(particles[i].x, particles[i].y);
                        ctx.lineTo(particles[j].x, particles[j].y);
                        const opacity = 1 - (distance / connectionDistance);
                        ctx.strokeStyle = `rgba(123, 47, 247, ${opacity * 0.3})`;
                        ctx.lineWidth = 1;
                        ctx.stroke();
                    }
                }
            }

            requestAnimationFrame(animate);
        }

        animate();

        window.addEventListener('resize', () => {
            canvas.width = window.innerWidth;
            canvas.height = window.innerHeight * 0.6;
        });
    </script>
</body>
</html>
